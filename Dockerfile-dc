ARG BASE_CONTAINER=define_as_build_arg
FROM $BASE_CONTAINER
LABEL MAINTAINER="CSEL Ops <admin@cs.colorado.edu>"

USER root

# Fix DL4006
SHELL ["/bin/bash", "-o", "pipefail", "-c"]

USER root

# Spark dependencies
ENV APACHE_SPARK_VERSION=3.0.1 \
    HADOOP_VERSION=3.2

RUN apt-get -y update && \
    apt-get install --no-install-recommends -y openjdk-8-jdk-headless ca-certificates-java && \
    update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java && \
    rm -rf /var/lib/apt/lists/*

# Using the preferred mirror to download Spark
WORKDIR /tmp

# hadolint ignore=SC2046
RUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\?as_json | \
    python -c "import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])") && \
    echo "e8b47c5b658e0fbc1e57eea06262649d8418ae2b2765e44da53aaf50094877d17297cc5f0b9b35df2ceef830f19aa31d7e56ead950bbe7f8830d6874f88cfc3c *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" | sha512sum -c - && \
    tar xzf "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /usr/local --owner root --group root --no-same-owner && \
    rm "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

WORKDIR /usr/local
RUN ln -s "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" spark

# Configure Spark
ENV SPARK_HOME=/usr/local/spark
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip \
    SPARK_OPTS="--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info" \
    PATH=$PATH:$SPARK_HOME/bin

USER $NB_UID

# Install pyarrow
RUN conda install --quiet -y 'pyarrow' && \
    conda clean --all -f -y && \
    fix-permissions "${CONDA_DIR}" && \
    fix-permissions "/home/${NB_USER}"

##
## Now the all-spark-notebook
##

USER root

# RSpark config
ENV R_LIBS_USER $SPARK_HOME/R/lib
RUN fix-permissions $R_LIBS_USER

# R pre-requisites
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    fonts-dejavu \
    gfortran \
    gcc && \
    rm -rf /var/lib/apt/lists/*

# R packages
# from https://github.com/jupyter/docker-stacks/blob/master/all-spark-notebook/Dockerfile
RUN conda install --quiet --yes \
    'r-base=4.0.3' \
    'r-ggplot2=3.3*' \
    'r-irkernel=1.1*' \
    'r-rcurl=1.98*' \
    'r-sparklyr=1.5*' \
    && \
    conda clean --all -f -y && \
    fix-permissions "${CONDA_DIR}" && \
    fix-permissions "/home/${NB_USER}"

# Spylon-kernel
RUN conda install --quiet --yes 'spylon-kernel=0.4*' && \
    conda clean --all -f -y && \
    python -m spylon_kernel install --sys-prefix && \
    rm -rf "/home/${NB_USER}/.local" && \
    fix-permissions "${CONDA_DIR}" && \
    fix-permissions "/home/${NB_USER}"

##
## Install hadoop
##

ENV APACHE_HADOOP_VERSION=3.2.1
RUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/hadoop/core/hadoop-${APACHE_HADOOP_VERSION}/hadoop-${APACHE_HADOOP_VERSION}.tar.gz\?as_json | \
    python -c "import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])")

RUN echo "d62709c3d7144fcaafc60e18d0fa03d7d477cc813e45526f3646030cd87dbf010aeccf3f4ce795b57b08d2884b3a55f91fe9d74ac144992d2dfe444a4bbf34ee *hadoop-${APACHE_HADOOP_VERSION}.tar.gz" | sha512sum -c - && \
    tar xzf "hadoop-${APACHE_HADOOP_VERSION}.tar.gz" -C /usr/local --owner $NB_UID --group $NB_GID --no-same-owner && \
    rm "hadoop-${APACHE_HADOOP_VERSION}.tar.gz" && \
    fix-permissions "/usr/local/hadoop-3.2.1"

##
## Configure HADOOP
##
ENV	 HADOOP_HOME=/usr/local/hadoop-3.2.1
ENV	 HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop \
	 HADOOP_INSTALL=$HADOOP_HOME \
	 HADOOP_MAPRED_HOME=$HADOOP_HOME \
	 HADOOP_COMMON_HOME=$HADOOP_HOME \
	 HADOOP_HDFS_HOME=$HADOOP_HOME \
	 YARN_HOME=$HADOOP_HOME \ 
	 HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native \
	 HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native" \
	 PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin \
	 JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
	 PATH=$PATH:$HADOOP_HOME/bin

RUN	 hdfs namenode -format && \
	 fix-permissions "/usr/local/hadoop-3.2.1"

##
## Install Google cloud utilities -- see https://cloud.google.com/sdk/docs/quickstart
##
RUN	echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \	
	apt-get -y install apt-transport-https ca-certificates gnupg && \
	curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \
	apt-get update && sudo apt-get -y install google-cloud-sdk kubectl 
	
RUN	pip install grpcio grpcio-tools && \
	apt-get -y install libprotobuf-dev protobuf-compiler libgrpc++-dev libgrpc-dev protobuf-compiler-grpc

#RUN	apt-get -y install rabbitmq-server

USER $NB_UID


WORKDIR $HOME
